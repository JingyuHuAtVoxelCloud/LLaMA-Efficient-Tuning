stage: sft
model_name_or_path: /mnt/local/jyhu/llm_assets/models/Yi-6B
template: default
do_train: true
dataset: glaive_toolcall,alpaca_gpt4_en,alpaca_gpt4_zh,oaast_sft_zh
dataset_dir: /mnt/local/jyhu/LLaMA-Efficient-Tuning/data
finetuning_type: lora
lora_target: all
lora_rank: 8
lora_dropout: 0.1
flash_attn: True
cutoff_len: 1024
output_dir: /mnt/local/jyhu/LLaMA-Efficient-Tuning/outputs/agent/yi-6b-agent
overwrite_cache: true
per_device_train_batch_size: 4
gradient_accumulation_steps: 4
lr_scheduler_type: cosine
logging_steps: 100
save_steps: 100
learning_rate: 0.00005 # 5e-5
num_train_epochs: 2.0
plot_loss: true
fp16: true
evaluation_strategy: steps
load_best_model_at_end: true
val_size: 0.001
preprocessing_num_workers: 32
overwrite_output_dir: true
# max_samples: 200000